# Configuraci√≥n de Ollama con GPU AMD

## ‚úÖ Completado

### 1. Docker Compose Actualizado
- ‚úÖ Servicio Ollama agregado con soporte GPU AMD
- ‚úÖ Volumen persistent para modelos (`ollama_data`)
- ‚úÖ Acceso a dispositivos GPU AMD (`/dev/kfd`, `/dev/dri`)
- ‚úÖ Backend ahora depende de Ollama
- ‚úÖ Health check configurado

### 2. Modelo Descargado
- ‚úÖ `llama3.2:3b` descargado en Ollama
- ‚úÖ Prueba b√°sica exitosa: ‚ú® **"La capital de Guatemala es Ciudad de Guatemala"**

### 3. Configuraci√≥n Backend
- ‚úÖ URL actualizada: `http://ollama:11434`
- ‚úÖ Modelo: `llama3.2:3b`
- ‚úÖ OllamaLLMAdapter listo para usar

## üöÄ C√≥mo Usar

### Iniciar todo (backend + frontend + db + ollama)
```bash
docker-compose up -d
```

### Ver logs de Ollama
```bash
docker logs proyecto_guate_ollama
```

### Probar Ollama directamente
```bash
# Desde el host (Ollama expuesto en puerto 11434)
curl http://localhost:11434/api/tags

# Generar una respuesta
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "¬øCu√°l es la capital de Guatemala?"
}'
```

### Probar integraci√≥n backend
```bash
python3 test_integration_ollama.py
```

### Hacer una b√∫squeda desde el frontend
1. Ir a http://localhost (frontend)
2. Escribir una b√∫squeda natural como:
   - "casas baratas en zona 10"
   - "departamentos con 3 habitaciones"
   - "terrenos disponibles"
3. El LLM convertir√° la b√∫squeda a SQL y ejecutar√° la query

## üìä Informaci√≥n de GPU

### Actual: CPU Mode
```
compute: cpu library
total: 46.8 GiB
available: 36.2 GiB
```

### Para habilitar GPU AMD cuando est√© disponible:
1. Aseg√∫rate de tener ROCm instalado en el host
2. El contenedor detectar√° autom√°ticamente `/dev/kfd` y `/dev/dri`
3. Ollama usar√° GPU AMD autom√°ticamente

## üîß Configuraci√≥n para GPU AMD (cuando est√© disponible)

En `docker-compose.yml` ya est√° configurado:
```yaml
devices:
  - /dev/kfd:/dev/kfd      # AMD GPU kernel interface
  - /dev/dri:/dev/dri      # AMD GPU driver interface
```

## üìÅ Archivos Modificados

- `docker-compose.yml` - Agregado servicio Ollama
- `backend/app/config.py` - URL Ollama actualizada
- `test_ollama.py` - Script de prueba b√°sico
- `test_integration_ollama.py` - Script de prueba de integraci√≥n backend

## ‚ö° Performance

El modelo `llama3.2:3b` es muy ligero:
- Tama√±o: ~2GB
- RAM requerida: ~6GB
- Tiempo respuesta: ~2-5 segundos en CPU
- Con GPU AMD: ~0.5-1 segundo

## üõ†Ô∏è Troubleshooting

### Ollama no inicia
```bash
docker logs proyecto_guate_ollama
```

### Backend no puede conectar a Ollama
- Verificar que Ollama est√© corriendo: `docker ps | grep ollama`
- Verificar que est√© en la misma red: `docker network ls`
- Probar conectividad desde backend: `docker exec proyecto_guate_backend curl http://ollama:11434/api/tags`

### GPU no detectada
- Verificar disponibilidad: `docker exec proyecto_guate_ollama rocm-smi`
- Verificar permisos de dispositivos: `ls -la /dev/kfd /dev/dri`

## üìö Referencias

- Ollama Docker: https://hub.docker.com/r/ollama/ollama
- Modelos disponibles: https://ollama.ai
- ROCm documentation: https://rocmdocs.amd.com/
